---
title: "Prediction Models of a Weight Lifting Exercise"
author: "Endless-Void"
date: "7/26/2020"
output: 
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction.

Devices like "Jawbone Up", "Nike FuelBand", and "Fitbit" allow us to to quantitatively analyze the movement of the people who wears it, this can be use to learn about the mistakes maded in certains exercises, in this case we will analyse and contruct a prediction model of the performance of a weight lifting exercise. 

## Analysis process.

### Used libraries. 

For the base code we use the next packages.

```{r libraries, message=FALSE, warning=FALSE}
library(ggplot2)
library(caret)
library(dplyr)
library(ggpubr)
```

### Data Obtention and Reading. 

```{r loading, message=FALSE, warning=FALSE}
if(!file.exists("./data")){dir.create("./data")}
GeneralFiles <- list.files("./data", full.names = TRUE)
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("./data/training.csv")){
        download.file(fileUrl1, destfile = "./data/training.csv", method = "curl")
}
if(!file.exists("./data/testing.csv")){
        download.file(fileUrl2, destfile = "./data/testing.csv", method = "curl")
}
training <- read.csv("./data/training.csv", header = TRUE)
testing <- read.csv("./data/testing.csv", header = TRUE)
```


### Pre-processing Data Sets. 

A very easy way to see initial problems in the data sets its the "str" function.

```{r strfun, warning=FALSE,message=FALSE, size=10}
str(training, list.len = 20)
```

Note that we have a lot of variables with empty info and with NA values also some variables that dosent have relevant use for the machine learning part (the first 7 columns), so wee have to get rid of all of them.

First the NA Values.

```{r NArmv, message=FALSE, warning=FALSE}
NAbyCol <- apply(training, 2, function(x) sum(is.na(x))) 
NACol <- names(subset(NAbyCol, NAbyCol == 19216))
training <- select(training, !NACol) 
dim(training)
```

Then the empty and unnecessary columns. 

```{r Emptyrmv, message=FALSE, warning=FALSE}
training <- select(training, !names(training)[1:7]) 
vars2 <- grepl("kurtosis|skewness|amplitude|min|max", names(training))
training <- select(training, c(names(training[,!vars2]), classe))
dim(training)
```

### Cross Validation.

```{r cvstep, warning=FALSE, message=FALSE}
set.seed(2549)
inTrain <- createDataPartition(y=training$classe,
                               p = 0.75, list = FALSE)
training <- training[inTrain,]
TEST <- training[-inTrain,]
dim(training)
dim(TEST)
```

### Exploration Analysis.

```{r plots, echo=FALSE, warning=FALSE, message=FALSE}
plot1 <- ggplot(training, aes(total_accel_arm, total_accel_dumbbell)) + 
        geom_point(aes(color = classe) , alpha = 0.3) + 
        scale_color_manual("Classe", 
                           values = c("A" = "red", 
                                      "B" = "blue",
                                      "C" = "yellow",
                                      "D" = "green",
                                      "E" = "purple"))

plot3 <- ggplot(training, aes(total_accel_arm, total_accel_forearm)) + 
        geom_point(aes(color = classe) , alpha = 0.3) + 
        scale_color_manual("Classe", 
                           values = c("A" = "red", 
                                      "B" = "blue",
                                      "C" = "yellow",
                                      "D" = "green",
                                      "E" = "purple"))

plot2 <- ggplot(training, aes(total_accel_forearm, total_accel_dumbbell)) + 
        geom_point(aes(color = classe) , alpha = 0.3) + 
        scale_color_manual("Classe", 
                           values = c("A" = "red", 
                                      "B" = "blue",
                                      "C" = "yellow",
                                      "D" = "green",
                                      "E" = "purple"))
plot4 <- ggplot(training, aes(magnet_arm_x, magnet_arm_y)) + 
        geom_point(aes(color = classe) , alpha = 0.3) + 
        scale_color_manual("Classe", 
                           values = c("A" = "red", 
                                      "B" = "blue",
                                      "C" = "yellow",
                                      "D" = "green",
                                      "E" = "purple"))

figure <- ggarrange(plot1, plot2, plot3, plot4, 
          labels = c("A", "B", "C", "D"),
          ncol = 2, nrow = 2)
annotate_figure(figure,
                top = text_grob("Total Accelerations Interactions", color = "black", face = "bold", size = 14))
```
**Figure 1.A to 1.C => Show the bidimensional interaction between the total acceleration recorded by the sensor of the arm, forearm and dumbell**

**Figure 1.D => Its an representative example of the bidimensional (X-Y plane) behavior of one of the sensors, in this case its from the arm**

We can see two major things with this figure:

- A linear model can not explain this behavior.
- A Cluster analysis it's a waste of time.

### Training Step. 

For computational impairments, we will restric the internal iterations of some methods to avoid memory problems. we will use 2 iterations and a simple cross validation method for the internal resampling method in each strategy of training. 

```{r iterations, message=FALSE, warning=FALSE}
IteControl <- trainControl(method='cv', number = 2) 
```

In order we will do:

1. Trees.
2. Trees with a pre-processing step using cluster analysis by the k-means method.
3. Random Forest.
4. Boosting with the tree method.

```{r fits, warning=FALSE, message=FALSE}
fit1 <- train(classe ~ ., method = "rpart", data = training)
fit1_1 <- train(classe ~ ., method = "rpart", preProcess = "knnImpute",data = training)
fit2 <- train(classe ~ ., method = "rf", trControl=IteControl, data = training, verbose=FALSE)
fit3 <- train(classe ~ ., method = "treebag", trControl=IteControl, data = training, verbose=FALSE)
```

For each one we have a initial Accuracy of: 

1. `r round(fit1$results[1,2] * 100, 2)`%
2. `r round(fit1_1$results[1,2] * 100, 2)`% **Here we can see that a cluster analysis 
indeed it is a waste of time**
3. `r round(fit2$results[2,2] * 100, 2)`%
4. `r round(fit3$results[1,2] * 100, 2)`%

### Selecting the best Method. 

using our testing data set we get: 

1. Trees.

```{r mthd1, message=FALSE, warning=FALSE}
confusionMatrix(as.factor(TEST$classe),predict(fit1, TEST))
```

2. Trees with a pre-processing step using cluster analysis by the k-means method.

```{r mthd1_1, message=FALSE, warning=FALSE}
confusionMatrix(as.factor(TEST$classe),predict(fit1_1, TEST))
```

3. Random Forest.

```{r mthd2, message=FALSE, warning=FALSE}
confusionMatrix(as.factor(TEST$classe),predict(fit2, TEST))
```

4. Boosting with the tree method.

```{r mthd3, message=FALSE, warning=FALSE}
confusionMatrix(as.factor(TEST$classe),predict(fit3, TEST))
```

We can se that the first two models have problems to predict the Class D, and the best one is the **Random Forest Method**

### Model Predictions.

Now we will use formally our Random Forest method to predict the class of 20 Weight lifting Exercises.

```{r predictions, message=FALSE, warning=FALSE}
pred1 <- predict(fit1, testing)
pred1_1 <- predict(fit1_1, testing)
pred2 <- predict(fit2, testing)
pred3 <- predict(fit3, testing)
data.frame(Trees = pred1, Trees_kmeans = pred1_1, Random_Forest = pred2, Boosting_Trees = pred3)
pred2 ## Most accurate answer 
```


